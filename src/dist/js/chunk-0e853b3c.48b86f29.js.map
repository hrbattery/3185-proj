{"version":3,"sources":["webpack:///./src/components/Header.vue?c495","webpack:///src/components/Header.vue","webpack:///./src/components/Header.vue?4c35","webpack:///./src/components/Header.vue","webpack:///./src/assets/images/新的.png","webpack:///./src/assets/images/GAN损失函数.png","webpack:///./src/pages/Page2.vue?00a4","webpack:///src/pages/Page2.vue","webpack:///./src/pages/Page2.vue?2212","webpack:///./src/pages/Page2.vue","webpack:///./src/assets/images/CNN网络结构.png","webpack:///./src/assets/images/修复前.png","webpack:///./src/assets/images/最后一张.png","webpack:///./src/assets/images/修复后.png"],"names":["render","_vm","this","_h","$createElement","_c","_self","attrs","_v","_l","name","key","headerTitles","indexOf","_s","on","darkModeTrigger","staticRenderFns","data","methods","$vuetify","theme","dark","component","module","exports","staticClass"],"mappings":"kHAAA,IAAIA,EAAS,WAAa,IAAIC,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,YAAY,CAACE,MAAM,CAAC,aAAa,SAAS,CAACF,EAAG,kBAAkB,CAACA,EAAG,QAAQ,CAACE,MAAM,CAAC,KAAO,KAAK,CAACF,EAAG,cAAc,CAACE,MAAM,CAAC,GAAK,MAAM,CAACF,EAAG,SAAS,CAACJ,EAAIO,GAAG,eAAe,IAAI,IAAI,GAAGH,EAAG,kBAAkBJ,EAAIQ,GAAIR,EAAgB,cAAE,SAASS,GAAM,OAAOL,EAAG,QAAQ,CAACM,IAAID,EAAKH,MAAM,CAAC,KAAO,KAAK,CAACF,EAAG,cAAc,CAACE,MAAM,CAAC,GAAM,KAAON,EAAIW,aAAaC,QAAQH,GAAM,KAAM,CAACT,EAAIO,GAAGP,EAAIa,GAAGJ,OAAU,MAAK,GAAGL,EAAG,YAAYA,EAAG,QAAQ,CAACE,MAAM,CAAC,KAAO,KAAK,CAACF,EAAG,cAAc,CAACE,MAAM,CAAC,GAAK,UAAU,CAACF,EAAG,SAAS,CAACJ,EAAIO,GAAG,sBAAsB,IAAI,GAAGH,EAAG,QAAQ,CAACE,MAAM,CAAC,KAAO,KAAK,CAACF,EAAG,SAAS,CAACU,GAAG,CAAC,MAAQd,EAAIe,kBAAkB,CAACf,EAAIO,GAAG,2BAA2B,IAAI,IAC5uBS,EAAkB,GC2BtB,GACEC,KADF,WAEI,MAAO,CACLN,aAAc,CACpB,eACA,mBACA,iBACA,sBAIEO,QAAS,CACPH,gBADJ,WAEMd,KAAKkB,SAASC,MAAMC,MAAQpB,KAAKkB,SAASC,MAAMC,QCzC0R,I,YCO5UC,EAAY,eACd,EACAvB,EACAiB,GACA,EACA,KACA,KACA,MAIa,OAAAM,E,8BClBfC,EAAOC,QAAU,IAA0B,uB,uBCA3CD,EAAOC,QAAU,IAA0B,4B,2CCA3C,IAAIzB,EAAS,WAAa,IAAIC,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,QAAQ,CAACE,MAAM,CAAC,GAAK,YAAY,CAACF,EAAG,UAAUA,EAAG,SAAS,CAACA,EAAG,cAAc,CAACA,EAAG,MAAM,CAACqB,YAAY,cAAc,CAACrB,EAAG,KAAK,CAACJ,EAAIO,GAAG,sBAAsBH,EAAG,MAAMJ,EAAIO,GAAG,OAAOH,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,YAAY,CAACN,EAAIO,GAAG,8BAA8BP,EAAIO,GAAG,KAAKH,EAAG,MAAMJ,EAAIO,GAAG,OAAOH,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,gBAAgB,CAACN,EAAIO,GAAG,qCAAqCP,EAAIO,GAAG,KAAKH,EAAG,MAAMJ,EAAIO,GAAG,OAAOH,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,UAAU,CAACN,EAAIO,GAAG,+BAA+BP,EAAIO,GAAG,KAAKH,EAAG,MAAMJ,EAAIO,GAAG,OAAOH,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,aAAa,CAACN,EAAIO,GAAG,mCAAmCP,EAAIO,GAAG,KAAKH,EAAG,MAAMA,EAAG,KAAKA,EAAG,KAAK,CAACE,MAAM,CAAC,GAAK,WAAW,CAACN,EAAIO,GAAG,gCAAgCH,EAAG,IAAI,CAACE,MAAM,CAAC,GAAK,UAAU,CAACN,EAAIO,GAAG,odAAodH,EAAG,IAAI,CAACJ,EAAIO,GAAG,icAAicH,EAAG,IAAI,CAACJ,EAAIO,GAAG,6SAA6SH,EAAG,IAAI,CAACJ,EAAIO,GAAG,+8BAA+8BH,EAAG,MAAM,CAACE,MAAM,CAAC,IAAM,EAAQ,WAA4CF,EAAG,IAAI,CAACJ,EAAIO,GAAG,0CAA0CH,EAAG,MAAMA,EAAG,MAAM,CAACE,MAAM,CAAC,IAAM,EAAQ,QAA4B,MAAQ,MAAM,OAAS,SAASF,EAAG,MAAM,CAACE,MAAM,CAAC,IAAM,EAAQ,QAA4B,MAAQ,MAAM,OAAS,SAASF,EAAG,KAAK,CAACE,MAAM,CAAC,GAAK,eAAe,CAACN,EAAIO,GAAG,wCAAwCH,EAAG,MAAM,CAACE,MAAM,CAAC,IAAM,EAAQ,WAA8BF,EAAG,KAAK,CAACJ,EAAIO,GAAG,iCAAiCH,EAAG,IAAI,CAACJ,EAAIO,GAAG,6HAA6HH,EAAG,IAAI,CAACJ,EAAIO,GAAG,00EAA00EH,EAAG,IAAI,CAACJ,EAAIO,GAAG,srDAAsrDH,EAAG,IAAI,CAACA,EAAG,MAAM,CAACE,MAAM,CAAC,IAAM,EAAQ,QAAgC,MAAQ,MAAM,OAAS,SAASN,EAAIO,GAAG,KAAKH,EAAG,MAAMJ,EAAIO,GAAG,k7BAAk7BH,EAAG,IAAI,CAACJ,EAAIO,GAAG,4bAA4bH,EAAG,KAAK,CAACJ,EAAIO,GAAG,mDAAmDH,EAAG,IAAI,CAACJ,EAAIO,GAAG,8+HAA8+HH,EAAG,KAAK,CAACJ,EAAIO,GAAG,2BAA2BH,EAAG,IAAI,CAACJ,EAAIO,GAAG,inBAAinBH,EAAG,MAAM,CAACE,MAAM,CAAC,IAAM,EAAQ,WAAmCN,EAAIO,GAAG,KAAKH,EAAG,MAAMA,EAAG,IAAI,CAACJ,EAAIO,GAAG,gRAAgRH,EAAG,MAAM,CAACE,MAAM,CAAC,IAAM,EAAQ,QAA4B,MAAQ,MAAM,OAAS,SAASF,EAAG,IAAI,CAACJ,EAAIO,GAAG,irEAAirEH,EAAG,IAAI,CAACJ,EAAIO,GAAG,qeAAqeH,EAAG,IAAI,CAACJ,EAAIO,GAAG,6iDAA6iDH,EAAG,IAAI,CAACJ,EAAIO,GAAG,mSAAmSH,EAAG,IAAI,CAACJ,EAAIO,GAAG,mEAAmEH,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,cAAc,OAAS,WAAW,CAACN,EAAIO,GAAG,UAAUP,EAAIO,GAAG,2BAA2BH,EAAG,KAAK,CAACE,MAAM,CAAC,GAAK,SAAS,CAACN,EAAIO,GAAG,iCAAiCH,EAAG,IAAI,CAACJ,EAAIO,GAAG,wEAAwEH,EAAG,IAAI,CAACJ,EAAIO,GAAG,wEAAwEH,EAAG,IAAI,CAACJ,EAAIO,GAAG,yEAAyEH,EAAG,IAAI,CAACJ,EAAIO,GAAG,wEAAwEH,EAAG,MAAM,CAACE,MAAM,CAAC,IAAM,EAAQ,QAA6B,MAAQ,MAAM,OAAS,SAASF,EAAG,KAAK,CAACE,MAAM,CAAC,GAAK,YAAY,CAACN,EAAIO,GAAG,mCAAmCH,EAAG,IAAI,CAACJ,EAAIO,GAAG,yBAAyBH,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,kDAAkD,OAAS,WAAW,CAACN,EAAIO,GAAG,6BAA6B,IAAI,IACrznBS,EAAkB,G,YCmQtB,GACE,KADF,WAEI,MAAJ,CACM,aAAN,CACA,QACA,QACA,QACA,WAIE,QAAF,CACI,KADJ,cAKE,WAAF,CACI,OAAJ,QAEE,QAnBF,cCpQ+U,I,YCO3UM,EAAY,eACd,EACAvB,EACAiB,GACA,EACA,KACA,KACA,MAIa,aAAAM,E,gCClBfC,EAAOC,QAAU,IAA0B,4B,qBCA3CD,EAAOC,QAAU,IAA0B,wB,qBCA3CD,EAAOC,QAAU,IAA0B,yB,qBCA3CD,EAAOC,QAAU,IAA0B","file":"js/chunk-0e853b3c.48b86f29.js","sourcesContent":["var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('v-toolbar',{attrs:{\"max-height\":\"64px\"}},[_c('v-toolbar-title',[_c('v-btn',{attrs:{\"icon\":\"\"}},[_c('router-link',{attrs:{\"to\":\"/\"}},[_c('v-icon',[_vm._v(\"mdi-home\")])],1)],1)],1),_c('v-toolbar-items',_vm._l((_vm.headerTitles),function(name){return _c('v-btn',{key:name,attrs:{\"text\":\"\"}},[_c('router-link',{attrs:{\"to\":(\"p\" + (_vm.headerTitles.indexOf(name)+1))}},[_vm._v(_vm._s(name))])],1)}),1),_c('v-spacer'),_c('v-btn',{attrs:{\"icon\":\"\"}},[_c('router-link',{attrs:{\"to\":\"about\"}},[_c('v-icon',[_vm._v(\"mdi-information\")])],1)],1),_c('v-btn',{attrs:{\"icon\":\"\"}},[_c('v-icon',{on:{\"click\":_vm.darkModeTrigger}},[_vm._v(\"mdi-theme-light-dark\")])],1)],1)}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","<template>\n  <v-toolbar max-height=\"64px\">\n    <!-- <v-app-bar-nav-icon></v-app-bar-nav-icon> -->\n    <!--v-toolbar-title><router-link to=\"/\">Home Page</router-link></v-toolbar-title-->\n    <v-toolbar-title>\n      <v-btn icon>\n        <router-link to=\"/\">\n          <v-icon>mdi-home</v-icon>\n        </router-link>\n      </v-btn>\n    </v-toolbar-title>\n    <!-- <v-toolbar-items v-for=\"name in headerTitles\" :key=\"name\">{{name}}</v-toolbar-items> -->\n    <v-toolbar-items>\n      <v-btn text v-for=\"name in headerTitles\" :key=\"name\"><router-link :to=\"`p${headerTitles.indexOf(name)+1}`\">{{name}}</router-link></v-btn>\n    </v-toolbar-items>\n    <v-spacer></v-spacer>\n    <v-btn icon>\n      <router-link to=\"about\">\n        <v-icon>mdi-information</v-icon>\n      </router-link>\n    </v-btn>\n    <v-btn icon>\n        <v-icon @click=\"darkModeTrigger\">mdi-theme-light-dark</v-icon>\n    </v-btn>\n  </v-toolbar>\n</template>\n\n<script>\nexport default {\n  data() {\n    return {\n      headerTitles: [\n        'Introduction',\n        'Image Inpainting',\n        'Style Transfer',\n        'Motion detection'\n      ]\n    }\n  }, \n  methods: {\n    darkModeTrigger() {\n      this.$vuetify.theme.dark = !this.$vuetify.theme.dark\n    }\n  }\n}\n</script>\n\n<style>\n\n</style>","import mod from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Header.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Header.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./Header.vue?vue&type=template&id=5df12924&\"\nimport script from \"./Header.vue?vue&type=script&lang=js&\"\nexport * from \"./Header.vue?vue&type=script&lang=js&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  null,\n  null\n  \n)\n\nexport default component.exports","module.exports = __webpack_public_path__ + \"img/新的.b42f748f.png\";","module.exports = __webpack_public_path__ + \"img/GAN损失函数.f414dc0e.png\";","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('v-app',{attrs:{\"id\":\"inspire\"}},[_c('Header'),_c('v-main',[_c('v-container',[_c('div',{staticClass:\"text-start\"},[_c('h1',[_vm._v(\"Image Inpainting\")]),_c('br'),_vm._v(\" 1.\"),_c('a',{attrs:{\"href\":\"#whatis\"}},[_vm._v(\"What is Image inpainting\")]),_vm._v(\" \"),_c('br'),_vm._v(\" 2.\"),_c('a',{attrs:{\"href\":\"#techniques\"}},[_vm._v(\"Techniques of Image Restoration\")]),_vm._v(\" \"),_c('br'),_vm._v(\" 3.\"),_c('a',{attrs:{\"href\":\"#tree\"}},[_vm._v(\"Image Inpainting Datasets\")]),_vm._v(\" \"),_c('br'),_vm._v(\" 4.\"),_c('a',{attrs:{\"href\":\"#example\"}},[_vm._v(\"an example of image inpaining\")]),_vm._v(\" \"),_c('br'),_c('p'),_c('h3',{attrs:{\"id\":\"whatis\"}},[_vm._v(\" What is Image Restoration\")]),_c('p',{attrs:{\"id\":\"para1\"}},[_vm._v(\" Image restoration is the operation of taking a corrupt/noisy image and estimating the clean, original image. Corruption may come in many forms such as motion blur, noise and camera mis-focus. Image restoration is performed by reversing the process that blurred the image and such is performed by imaging a point source and use the point source image, which is called the Point Spread Function (PSF) to restore the image information lost to the blurring process.\")]),_c('p',[_vm._v(\" Image restoration is different from image enhancement in that the latter is designed to emphasize features of the image that make the image more pleasing to the observer, but not necessarily to produce realistic data from a scientific point of view. Image enhancement techniques (like contrast stretching or de-blurring by a nearest neighbor procedure) provided by imaging packages use no a priori model of the process that created the image.\")]),_c('p',[_vm._v(\" With image enhancement noise can effectively be removed by sacrificing some resolution, but this is not acceptable in many applications. In a fluorescence microscope, resolution in the z-direction is bad as it is. More advanced image processing techniques must be applied to recover the object.\")]),_c('p',[_vm._v(\" The objective of image restoration techniques is to reduce noise and recover resolution loss Image processing techniques are performed either in the image domain or the frequency domain. The most straightforward and a conventional technique for image restoration is deconvolution, which is performed in the frequency domain and after computing the Fourier transform of both the image and the PSF and undo the resolution loss caused by the blurring factors. This deconvolution technique, because of its direct inversion of the PSF which typically has poor matrix condition number, amplifies noise and creates an imperfect deblurred image. Also, conventionally the blurring process is assumed to be shift-invariant. Hence more sophisticated techniques, such as regularized deblurring, have been developed to offer robust recovery under different types of noises and blurring functions. It is of 3 types: 1. Geometric correction 2. radiometric correction 3. noise removal\")]),_c('img',{attrs:{\"src\":require(\"../assets/images/Image Inpainting.jpg\")}}),_c('p',[_vm._v(\"Figure 1. kinds of images inpainting\")]),_c('br'),_c('img',{attrs:{\"src\":require(\"../assets/images/修复前.png\"),\"width\":\"200\",\"height\":\"320\"}}),_c('img',{attrs:{\"src\":require(\"../assets/images/修复后.png\"),\"width\":\"200\",\"height\":\"320\"}}),_c('h2',{attrs:{\"id\":\"techniques\"}},[_vm._v(\" 2.Techniques of Image Restoration\")]),_c('img',{attrs:{\"src\":require(\"../assets/images/新的.png\")}}),_c('h4',[_vm._v(\"2.1Sequential-Based Methods\")]),_c('p',[_vm._v(\"Approaches related to images inpainting can be classified into two categories: patch-based and diffusion-based methods.\")]),_c('p',[_vm._v(\"Patch-based methods are based on techniques that fill in the missing region patch-bypatch by searching for well-matching replacement patches (i.e., candidate patches) in the undamaged part of the image and copying them to corresponding locations. Many methods have been proposed for image inpainting using patch-based method. Ruži´c and Pi˘zurica [15] proposed a patch-based method consisting of searching the well-matched patch in the texture component using Markov random field (MRF). Jin and Ye [16] proposed a patch-based approach based on annihilation property filter and low rank structured matrix. In order to remove an object from an image, Kawai et al. [17] proposed an approach based on selecting the target object and limiting the search around the target by the background. Using two-stage low rank approximation (TSLRA) [18] and gradient-based low rank approximation [19], authors proposed patch-based methods for recovering the corrupted block in the image. On RGB-D images full of noise and text, Xue et al. [20] proposed a depth image inpainting method based on Low Gradient Regularizatio n. Liu et al. [21] used the statistical regularization and similarity between regions to extract dominant linear structures of target regions followed by repairing the missing regions using Markov random field model (MRF). Ding et al. [22] proposed a patch-based method for image inpainting using Nonlocal Texture Matching and Nonlinear Filtering (Alpha-trimmed mean filter). Duan et al. [23] proposed an image inpainting approach based on the Non-Local Mumford–Shah model (NL–MS). Fan and Zhang [24] proposed another image inpainting method based on measuring the similarity between patches using the Sum of Squared Differences (SSD). In order to remove blocks from an image, Jiang [25] proposed a method for image compression. Using Singular value decomposition and an approximation matrix, Alilou and Yaghmaee [26] proposed an approach to reconstruct the missing regions. Other notable research includes using texture analysis on Thangka images to recover missing block in an image [27] and using the structure information of images [28,29]. In the same context, Zeng et al. [30] proposed the use of Saliency Map and Gray entropy. Zhang et al. [31] proposed an image inpainting method using a joint probability density matrix (JPDM) for object removal from images.\")]),_c('p',[_vm._v(\"Wali et al. [32] proposed a denoising and inpainting method using total generalized variation (TGV). The authors analyze three types of distortion including text, noise, masks. In the same context, Zhang et al. [33] proposed an example-based image inpainting approach based on color distribution by restoring the missed regions using the neighboring regions. This work analyses the many types of distortions including objects, text, and scratch. The multiscale graph cuts technique is used for inpainting images in [34] by analyzing different types of distortion. In [35] the authors proposed a novel joint data-hiding and compression scheme for digital images using side match vector quantization (SMVQ) and image inpainting. The proposed approach is tested on six grayscale recognized images including Lena, airplane, peppers, sailboat, lake, and Tiffany. In order to preserving the texture consistency and structure coherence, the authors in [36] remove the added objects in the images using multiple pyramids method, local patch statistics and geometric feature-based sparse representation. For 3D stacked image sensor the authors in [37] proposed an image inpainting method using discrete wavelet transform (DWT). In order to fill the missed region, the authors in [38] proposed a patch-based method by search and fills-in these regions with the best matching information surrounding it. In the goal to reconstruct the Borehole images the authors in [39] proposed a method by analyzing the texture and structure component of the images. Helmholtz equation is used for inpainting images in [40], after the inpainting of the missed region the authors proposed a method for enhancing the quality of the images.\")]),_c('p',[_c('img',{attrs:{\"src\":require(\"../assets/images/CNN网络结构.png\"),\"width\":\"800\",\"height\":\"300\"}}),_vm._v(\" \"),_c('br'),_vm._v(\" Diffusion-based methods fill in the missing region (i.e. hole), by smoothly propagating image content from the boundary to the interior of the missing region. For that, Li et al. [41] proposed a diffusion-based method for image inpainting by localizing the diffusion of inpainted regions followed by the construction of a feature set based on the intra-channel and inter-channel local variances of the changes to identify the inpainted regions. Another diffusion-based method of image inpainting proposed by the same authors in a later research [42] involves exploiting diffusion coefficients which were computed using the distance and direction between the damaged pixel and its neighborhood pixel. Sridevi et al. [43] proposed another diffusion-based image inpainting method based on Fractional-order derivative and Fourier transform. Table 1 depicts a summary of patch-based and diffusion-based sequential methods for image inpainting. \")]),_c('p',[_vm._v(\"Jin et al. [44] proposed an approach called sparsity-based image inpainting detection based on canonical correlation analysis (CCA). Mo and Zhou [45] present a research-based on dictionary learning using sparse representation. These methods are robust for simple images, but when the image is complex like contains a lot of texture and object or the object cover a large region in the images, searching for similar patch can be difficult.\")]),_c('h4',[_vm._v(\"2.2Convolutional-Neural-Network-Based Methods\")]),_c('p',[_vm._v(\" Recently, the strong potential of deep convolutional networks (CNNs) is being exhibited in all computer vision tasks, especially in image inpainting. CNNs are used specifically in order to improve the expected results in this field using large-scale training data. The sequentialbased methods succeed in some parts of image inpainting like filling texture details with promising results, yet the problem of capturing the global structure is still a challenging task [46]. Several methods have been proposed for image inpainting using convolutional neural networks (CNNs) or encoder-decoder network based on CNN. Shift-Net based on U-Net architecture is one of these methods that recover the missing block with good accuracy in terms of structure and fine-detailed texture [46]. In the same context, Weerasekera et al. [47] use depth map of the image as input of the CNN architecture, whereas Zhao et al. [48] use the proposed architecture for inpainting X-ray medical images. VORNet [49] is another CNN-based approach for video inpainting for object removal. Most image inpainting methods know the reference of damaged pixels of blocks. Cai et al. [50] proposed a blind image inpainting method named (BICNN). Based on convolutional neural networks (CNNs) using encoder-decoder network structure many works have been proposed for image inpainting. Zhu et al. [51] proposed a patch-based inpainting method for forensics images. Using the same technique of encoder-decoder network, Sidorov and Hardeberg [52] proposed an architecture for denoising, inpainting, and super-resolution for noised, inpainted and low-resolution images, respectively. Zeng et al. [53] built a pyramidal-context architecture called PEN-NET for high-quality image inpainting. Liu et al. [54] proposed a layer to the encoder-decoder network called coherent semantic attention (SCA) layer for image inpainting method. This proposed architecture is presented in Fig. 3. Further, Pathak et al. [55] proposed encoder-decoder model for image inpainting. In order to fill the gap between lines drawing in an image, Sasaki et al. [56] used an encoder-decoder-based model. This work can be helpful for scanned data that can miss some parts. For the UAV data that can be affected in terms of resolution or containing some blindspots, Hsu et al. [57] proposed a solution using VGG architecture. Also, for removing some text from the images Nakamura et al. [58] proposed a text erasing method using CNN. In order to enhance the images of the damaged artwork, Xiang et al. [59] also proposed a CNN-based method. In the same context as [59] and using GRNN neural network, Alilou and Yaghmaee [60] proposed a non-texture image inpainting method. Unlike the previous methods, Liao et al. [61] proposed a method called Artist-Net for image inpainting. The same goal is reached by Cai et al. [62] who proposed a semantic object removal approach using CNN architecture. In order to remove motifs from single images, Hertz et al. [63] proposed a CNN-based approach. Table 2 summarizes the CNN-based methods with a description of the type of data used for image inpainting. For the same purpose of image inpainting, but for replacing a region of an image by another region from another image, the authors in [64] based on VGG model trained their own model. In order to mitigate the effect of the gradient disappearance, the authors in [65] introduce a dense block for U-Net architecture that is used for inpainting the images. For medical purposes, the authors in [67] attempted to denoising the medical images using the principle of image inpainting using Residual U-Net architecture. To address the blurring and color discrepancy problems for image inpainting the authors in [66] proposed a method for missed region reconstruction using region-wise convolutions. As the authors in [68] add some layers named Interleaved Zooming Block in the encoder-decoder architecture for impainting the images. The authors in [69] Proposed a full-resolution residual block (FRRB) with an encoder-decoder model for the same purpose. \")]),_c('h4',[_vm._v(\" 2.3GAN-Based Methods\")]),_c('p',[_vm._v(\" The much-used technique nowadays, was introduced for image generation in 2014 in [70]. Generative adversarial networks (GANs) are a framework which contains two feed-forward networks, a generator G and a discriminator D. The generative network, G, is trained to create a new image which is indistinguishable from real images, whereas a discriminative network, D is trained to differentiate between real and generated images. This relation can be considered as a two-player min-max game in which G and D compete. To this end, the G (D) tries to minimize (maximize) the loss function, i.e. adversarial loss, as follows: \")]),_c('img',{attrs:{\"src\":require(\"../assets/images/GAN损失函数.png\")}}),_vm._v(\" \"),_c('br'),_c('p',[_vm._v(\" where z and x denote a random noise vector and a real image sampled from the noise Pz(z) and real data distribution Pdata(x), respectively. Recently, the GAN has been applied to several semantic inpainting techniques in order to complete the hole region naturally. \")]),_c('img',{attrs:{\"src\":require(\"../assets/images/GAN.jpg\"),\"width\":\"700\",\"height\":\"320\"}}),_c('p',[_vm._v(\" GANs are a framework that contains two feed-forward networks, a generator G and a discriminator D, as shown in Fig. 4. The generator takes random noise z as input and generates some fake samples similar to real ones; while the discriminator has to learn to determine whether samples are real or fake. At present, Generative Adversarial Network (GAN) becomes the most used technique in all computer vision applications. GAN-based approaches use a coarse-to-fine network and contextual attention module gives good performance and is proven to be helpful for inpainting [71–75]. Existing image inpainting methods based on GAN are generally a few. Out of these, we find that in [71], Chen and Hu proposed a GAN-based semantic image inpainting method, named progressive inpainting, where a pyramid strategy from a low-resolution image to a higher one is performed for repairing the image. For handwritten images, Li et al. [72] proposed a method for inpainting and recognition of occluded characters. The methods use improved GoogLeNet and deep convolutional generative adversarial network (DCGAN). In an image inpainting method named PEPSI [76] the authors unify the two-stage cascade network of the coarse-to-fine network into a single-stage encoder-decoder network. Where PEPSI++ is the extended version of PEPSI [73]. In [74] the authors used Encoder-decoder network and multi-scale GAN for image inpainting. The same combination is used in [75] for image inpainting and image-to-image transformation purposes. On the RBG-D images, Dhamo et al. [77] used CNN and GAN model to generate the background of a scene by removing the object in the foreground image as performed by many methods of motion detection using background subtraction [78,79]. In order to complete the missing regions in the image, Vitoria et al. [80] proposed an improved version of the Wasserstein GAN with the incorporation of Discriminator and Generator architecture. In the same context, but on sea surface temperature (SST) images, the Dong et al. [81] proposed a deep convolutional generative adversarial network (DCGAN) for filing the missing parts of the images. Also, Lou et al. [82] exploit a modifier GAN architecture for image inpainting \")]),_c('p',[_vm._v(\" whereas, Salem et al. [83] proposed a semantic image inpainting method using adversarial loss and self-learning encoder-decoder model. A good image restoration method requires preserving structural consistency and texture clarity. For this reason, Liu et al. [84] proposed a GAN-based method for image inpainting on face images. FiNet [85] is another approach found in the literature for fashion image inpainting that consists of completing the missing parts in fashion images. \")]),_c('p',[_vm._v(\" Recently, several approaches are proposed by combining some additional techniques (GAN, CNN,…) for inpainting the images. Jiao et al. [86] combined an encoder-decoder, multi-layer convolutions layers and GAN for restoring the images. The authors in [87] proposed a two-stage adversarial model named EdgeConnect by providing a generator for edge followed by an image inpainting model. The first model attempt to provide an edge completion component and the second one, inpaint the RGB image. According to the fact that GAN-based image inpainting models do not care out to the consistency of the structural and textural values between the inpainted region and their neighboring, the authors in [88] attempts to handle this limitation by providing a GAN model for learning the alignment between the block around the restored region and the original region. For the same reason as [88], taking into consideration the semantic consistency between restored images and original images, Li et al. [89] provided a boosted GAN model comprising an inpainting network and a discriminative network. When the inpainting network discovers the segmentation information of the input images, the discriminative network discovers the regularizations of the overall realness and segmentation consistency with the originals images. In the same context and using GAN-based models for images inpainting, each work provides some prior processing on GAN networks to get the best inpainting results for different types of images including medical images [90], face images [91] or scenes images [92]. \")]),_c('p',[_vm._v(\" The GAN-based methods give a good addition to the performance of image inpainting algorithms, but the speed of training is lower and needs very good performance machines, and this is due to computational resources requirements including network parameters and convolution operations. \")]),_c('p',[_vm._v(\"There are many articles about application of Image Restoration \"),_c('a',{attrs:{\"href\":\"oldman.html\",\"target\":\"_blank\"}},[_vm._v(\"相关论文\")]),_vm._v(\" You can have a look!\")]),_c('h2',{attrs:{\"id\":\"tree\"}},[_vm._v(\"3.Image Inpainting Datasets\")]),_c('p',[_vm._v(\"Technique 1 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")]),_c('p',[_vm._v(\"Technique 2 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")]),_c('p',[_vm._v(\"Technique 3 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")]),_c('p',[_vm._v(\"Technique 4 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")]),_c('img',{attrs:{\"src\":require(\"../assets/images/最后一张.png\"),\"width\":\"580\",\"height\":\"320\"}}),_c('h2',{attrs:{\"id\":\"example\"}},[_vm._v(\"4.example of Image Inpainting\")]),_c('p',[_vm._v(\"For more information \"),_c('a',{attrs:{\"href\":\"https://en.wikipedia.org/wiki/Image_restoration\",\"target\":\"_blank\"}},[_vm._v(\"Visit Wikipedia!\")])])])])],1)],1)}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","<template>\n  <v-app id=\"inspire\">\n    <Header/>\n    <v-main>\n      <v-container>\n      <div class=\"text-start\">\n      <h1>Image Inpainting</h1><br>\n\n        1.<a href=\"#whatis\">What is Image inpainting</a>  <br>\n        2.<a href=\"#techniques\">Techniques of Image Restoration</a>  <br>\n        3.<a href=\"#tree\">Image Inpainting Datasets</a>  <br>\n        4.<a href=\"#example\">an example of image inpaining</a>  <br>\n        <p></p>\n       <h3 id = \"whatis\"> What is Image Restoration</h3>\n       <p id=\"para1\"> Image restoration is the operation of taking a corrupt/noisy image and estimating the clean, original image. \n            Corruption may come in many forms such as motion blur, noise and camera mis-focus. Image restoration is performed by \n            reversing the process that blurred the image and such is performed by imaging a point source and use the point source image, \n            which is called the Point Spread Function (PSF) to restore the image information lost to the blurring process.</p>\n\n        <p> Image restoration is different from image enhancement in that the latter is designed to emphasize features of the image that make the \n            image more pleasing to the observer, but not necessarily to produce realistic data from a scientific point of view. Image enhancement techniques \n            (like contrast stretching or de-blurring by a nearest neighbor procedure) provided by imaging packages \n            use no a priori model of the process that created the image.</p>\n\n        <p> With image enhancement noise can effectively be removed by sacrificing some resolution, but this is not acceptable in many applications. \n            In a fluorescence microscope, resolution in the z-direction is bad as it is. \n            More advanced image processing techniques must be applied to recover the object.</p>\n\n        <p> The objective of image restoration techniques is to reduce noise and recover resolution loss Image processing techniques \n            are performed either in the image domain or the frequency domain. The most straightforward \n            and a conventional technique for image restoration is deconvolution, which is performed in the frequency domain \n            and after computing the Fourier transform of both the image and the PSF and undo the resolution loss caused by the blurring factors. \n            This deconvolution technique, because of its direct inversion of the PSF which typically has poor matrix condition number, \n            amplifies noise and creates an imperfect deblurred image. Also, conventionally the blurring process is assumed to be shift-invariant. \n            Hence more sophisticated techniques, such as regularized deblurring, have been developed \n            to offer robust recovery under different types of noises and blurring functions. \n            It is of 3 types: 1. Geometric correction 2. radiometric correction 3. noise removal</p>\n\n        <img src=\"../assets/images/Image Inpainting.jpg\">\n        <p>Figure 1. kinds of images inpainting</p>\n        <br>\n        <img src=\"../assets/images/修复前.png\" width=\"200\" height=\"320\">\n        <img src=\"../assets/images/修复后.png\" width=\"200\" height=\"320\">\n        \n       <h2 id=\"techniques\"> 2.Techniques of Image Restoration</h2>\n        \n            \n        \n          <img src=\"../assets/images/新的.png\" >\n\n          <h4>2.1Sequential-Based Methods</h4>\n          <p>Approaches related to images inpainting can be classified into two categories: patch-based\n            and diffusion-based methods.</p>\n            <p>Patch-based methods are based on techniques that fill in the missing region patch-bypatch by searching for well-matching replacement patches (i.e., candidate patches) in the\n                undamaged part of the image and copying them to corresponding locations. Many methods\n                have been proposed for image inpainting using patch-based method. Ruži´c and Pi˘zurica [15]\n                proposed a patch-based method consisting of searching the well-matched patch in the texture component using Markov random field (MRF). Jin and Ye [16] proposed a patch-based\n                approach based on annihilation property filter and low rank structured matrix. In order to\n                remove an object from an image, Kawai et al. [17] proposed an approach based on selecting the target object and limiting the search around the target by the background. Using\n                two-stage low rank approximation (TSLRA) [18] and gradient-based low rank approximation [19], authors proposed patch-based methods for recovering the corrupted block in the\n                image. On RGB-D images full of noise and text, Xue et al. [20] proposed a depth image\n                inpainting method based on Low Gradient Regularizatio n. Liu et al. [21] used the statistical\n                regularization and similarity between regions to extract dominant linear structures of target\n                regions followed by repairing the missing regions using Markov random field model (MRF).\n                Ding et al. [22] proposed a patch-based method for image inpainting using Nonlocal Texture\n                Matching and Nonlinear Filtering (Alpha-trimmed mean filter). Duan et al. [23] proposed an\n                image inpainting approach based on the Non-Local Mumford–Shah model (NL–MS). Fan\n                and Zhang [24] proposed another image inpainting method based on measuring the similarity between patches using the Sum of Squared Differences (SSD). In order to remove\n                blocks from an image, Jiang [25] proposed a method for image compression. Using Singular\n                value decomposition and an approximation matrix, Alilou and Yaghmaee [26] proposed an\n                approach to reconstruct the missing regions. Other notable research includes using texture\n                analysis on Thangka images to recover missing block in an image [27] and using the structure information of images [28,29]. In the same context, Zeng et al. [30] proposed the use\n                of Saliency Map and Gray entropy. Zhang et al. [31] proposed an image inpainting method\n                using a joint probability density matrix (JPDM) for object removal from images.</p>\n\n\n                <p>Wali et al. [32] proposed a denoising and inpainting method using total generalized variation (TGV). The authors analyze three types of distortion including text, noise, masks. In the\n                    same context, Zhang et al. [33] proposed an example-based image inpainting approach based\n                    on color distribution by restoring the missed regions using the neighboring regions. This work\n                    analyses the many types of distortions including objects, text, and scratch. The multiscale\n                    graph cuts technique is used for inpainting images in [34] by analyzing different types of\n                    distortion. In [35] the authors proposed a novel joint data-hiding and compression scheme\n                    for digital images using side match vector quantization (SMVQ) and image inpainting. The\n                    proposed approach is tested on six grayscale recognized images including Lena, airplane,\n                    peppers, sailboat, lake, and Tiffany. In order to preserving the texture consistency and structure coherence, the authors in [36] remove the added objects in the images using multiple\n                    pyramids method, local patch statistics and geometric feature-based sparse representation.\n                    For 3D stacked image sensor the authors in [37] proposed an image inpainting method using\n                    discrete wavelet transform (DWT). In order to fill the missed region, the authors in [38]\n                    proposed a patch-based method by search and fills-in these regions with the best matching\n                    information surrounding it. In the goal to reconstruct the Borehole images the authors in\n                    [39] proposed a method by analyzing the texture and structure component of the images.\n                    Helmholtz equation is used for inpainting images in [40], after the inpainting of the missed\n                    region the authors proposed a method for enhancing the quality of the images.</p>\n                    <p>\n                        <img src=\"../assets/images/CNN网络结构.png\" width=\"800\" height=\"300\"> <br>\n                        Diffusion-based methods fill in the missing region (i.e. hole), by smoothly propagating\n                        image content from the boundary to the interior of the missing region. For that, Li et al.\n                        [41] proposed a diffusion-based method for image inpainting by localizing the diffusion of\n                        inpainted regions followed by the construction of a feature set based on the intra-channel\n                        and inter-channel local variances of the changes to identify the inpainted regions. Another\n                        diffusion-based method of image inpainting proposed by the same authors in a later research\n                        [42] involves exploiting diffusion coefficients which were computed using the distance and\n                        direction between the damaged pixel and its neighborhood pixel. Sridevi et al. [43] proposed\n                        another diffusion-based image inpainting method based on Fractional-order derivative and\n                        Fourier transform. Table 1 depicts a summary of patch-based and diffusion-based sequential\n                        methods for image inpainting.\n                    </p>\n                    <p>Jin et al. [44] proposed an approach called sparsity-based image inpainting detection\n                        based on canonical correlation analysis (CCA). Mo and Zhou [45] present a research-based\n                        on dictionary learning using sparse representation. These methods are robust for simple\n                        images, but when the image is complex like contains a lot of texture and object or the object\n                        cover a large region in the images, searching for similar patch can be difficult.</p>\n          <h4>2.2Convolutional-Neural-Network-Based Methods</h4>\n          <p>\n            Recently, the strong potential of deep convolutional networks (CNNs) is being exhibited in\n            all computer vision tasks, especially in image inpainting. CNNs are used specifically in order\n            to improve the expected results in this field using large-scale training data. The sequentialbased methods succeed in some parts of image inpainting like filling texture details with\n            promising results, yet the problem of capturing the global structure is still a challenging\n            task [46]. Several methods have been proposed for image inpainting using convolutional\n            neural networks (CNNs) or encoder-decoder network based on CNN. Shift-Net based on\n            U-Net architecture is one of these methods that recover the missing block with good accuracy in terms of structure and fine-detailed texture [46]. In the same context, Weerasekera\n            et al. [47] use depth map of the image as input of the CNN architecture, whereas Zhao et\n            al. [48] use the proposed architecture for inpainting X-ray medical images. VORNet [49] is\n            another CNN-based approach for video inpainting for object removal. Most image inpainting methods know the reference of damaged pixels of blocks. Cai et al. [50] proposed a\n            blind image inpainting method named (BICNN). Based on convolutional neural networks\n            (CNNs) using encoder-decoder network structure many works have been proposed for image\n            inpainting. Zhu et al. [51] proposed a patch-based inpainting method for forensics images.\n            Using the same technique of encoder-decoder network, Sidorov and Hardeberg [52] proposed an architecture for denoising, inpainting, and super-resolution for noised, inpainted\n            and low-resolution images, respectively. Zeng et al. [53] built a pyramidal-context architecture called PEN-NET for high-quality image inpainting. Liu et al. [54] proposed a layer\n            to the encoder-decoder network called coherent semantic attention (SCA) layer for image\n            inpainting method. This proposed architecture is presented in Fig. 3. Further, Pathak et al.\n            [55] proposed encoder-decoder model for image inpainting. In order to fill the gap between\n            lines drawing in an image, Sasaki et al. [56] used an encoder-decoder-based model. This\n            work can be helpful for scanned data that can miss some parts. For the UAV data that can\n            be affected in terms of resolution or containing some blindspots, Hsu et al. [57] proposed a\n            solution using VGG architecture. Also, for removing some text from the images Nakamura\n            et al. [58] proposed a text erasing method using CNN. In order to enhance the images of the\n            damaged artwork, Xiang et al. [59] also proposed a CNN-based method. In the same context\n            as [59] and using GRNN neural network, Alilou and Yaghmaee [60] proposed a non-texture image inpainting method. Unlike the previous methods, Liao et al. [61] proposed a method\n            called Artist-Net for image inpainting. The same goal is reached by Cai et al. [62] who proposed a semantic object removal approach using CNN architecture. In order to remove motifs\n            from single images, Hertz et al. [63] proposed a CNN-based approach. Table 2 summarizes\n            the CNN-based methods with a description of the type of data used for image inpainting.\n            For the same purpose of image inpainting, but for replacing a region of an image by\n            another region from another image, the authors in [64] based on VGG model trained their\n            own model. In order to mitigate the effect of the gradient disappearance, the authors in [65] introduce a dense block for U-Net architecture that is used for inpainting the images. For\n                medical purposes, the authors in [67] attempted to denoising the medical images using the\n            principle of image inpainting using Residual U-Net architecture. To address the blurring and\n            color discrepancy problems for image inpainting the authors in [66] proposed a method for\n            missed region reconstruction using region-wise convolutions. As the authors in [68] add some\n            layers named Interleaved Zooming Block in the encoder-decoder architecture for impainting\n            the images. The authors in [69] Proposed a full-resolution residual block (FRRB) with an\n            encoder-decoder model for the same purpose.\n          </p>\n\n          <h4> 2.3GAN-Based Methods</h4>\n          <p>\n            The much-used technique nowadays, was introduced for image generation in 2014 in [70].\n            Generative adversarial networks (GANs) are a framework which contains two feed-forward\n            networks, a generator G and a discriminator D. The generative network, G, is trained to\n            create a new image which is indistinguishable from real images, whereas a discriminative\n            network, D is trained to differentiate between real and generated images. This relation can\n            be considered as a two-player min-max game in which G and D compete. To this end, the G\n            (D) tries to minimize (maximize) the loss function, i.e. adversarial loss, as follows:\n          </p>\n          <img src=\"../assets/images/GAN损失函数.png\" > <br>\n          \n\n          <p>\n            where z and x denote a random noise vector and a real image sampled from the noise Pz(z) and\n            real data distribution Pdata(x), respectively. Recently, the GAN has been applied to several\n            semantic inpainting techniques in order to complete the hole region naturally.\n          </p>\n            <img src=\"../assets/images/GAN.jpg\" width=\"700\" height=\"320\">\n          <p>\n            GANs are a framework that contains two feed-forward networks, a generator G and a\n            discriminator D, as shown in Fig. 4. The generator takes random noise z as input and\n            generates some fake samples similar to real ones; while the discriminator has to learn to\n            determine whether samples are real or fake. At present, Generative Adversarial Network\n            (GAN) becomes the most used technique in all computer vision applications. GAN-based\n            approaches use a coarse-to-fine network and contextual attention module gives good performance and is proven to be helpful for inpainting [71–75]. Existing image inpainting methods\n            based on GAN are generally a few. Out of these, we find that in [71], Chen and Hu proposed a\n            GAN-based semantic image inpainting method, named progressive inpainting, where a pyramid strategy from a low-resolution image to a higher one is performed for repairing the image.\n            For handwritten images, Li et al. [72] proposed a method for inpainting and recognition of\n            occluded characters. The methods use improved GoogLeNet and deep convolutional generative adversarial network (DCGAN). In an image inpainting method named PEPSI [76] the\n            authors unify the two-stage cascade network of the coarse-to-fine network into a single-stage\n            encoder-decoder network. Where PEPSI++ is the extended version of PEPSI [73]. In [74]\n            the authors used Encoder-decoder network and multi-scale GAN for image inpainting. The\n            same combination is used in [75] for image inpainting and image-to-image transformation\n            purposes. On the RBG-D images, Dhamo et al. [77] used CNN and GAN model to generate\n            the background of a scene by removing the object in the foreground image as performed by\n            many methods of motion detection using background subtraction [78,79]. In order to complete the missing regions in the image, Vitoria et al. [80] proposed an improved version of the\n            Wasserstein GAN with the incorporation of Discriminator and Generator architecture. In the\n            same context, but on sea surface temperature (SST) images, the Dong et al. [81] proposed a\n            deep convolutional generative adversarial network (DCGAN) for filing the missing parts of\n            the images. Also, Lou et al. [82] exploit a modifier GAN architecture for image inpainting\n          </p>\n\n          <p>\n            whereas, Salem et al. [83] proposed a semantic image inpainting method using adversarial\n            loss and self-learning encoder-decoder model. A good image restoration method requires\n            preserving structural consistency and texture clarity. For this reason, Liu et al. [84] proposed\n            a GAN-based method for image inpainting on face images. FiNet [85] is another approach\n            found in the literature for fashion image inpainting that consists of completing the missing\n            parts in fashion images.\n          </p>\n\n          <p>\n            Recently, several approaches are proposed by combining some additional techniques\n            (GAN, CNN,…) for inpainting the images. Jiao et al. [86] combined an encoder-decoder,\n            multi-layer convolutions layers and GAN for restoring the images. The authors in [87] proposed a two-stage adversarial model named EdgeConnect by providing a generator for edge\n            followed by an image inpainting model. The first model attempt to provide an edge completion component and the second one, inpaint the RGB image. According to the fact that\n            GAN-based image inpainting models do not care out to the consistency of the structural\n            and textural values between the inpainted region and their neighboring, the authors in [88]\n            attempts to handle this limitation by providing a GAN model for learning the alignment\n            between the block around the restored region and the original region. For the same reason\n            as [88], taking into consideration the semantic consistency between restored images and\n            original images, Li et al. [89] provided a boosted GAN model comprising an inpainting network and a discriminative network. When the inpainting network discovers the segmentation\n            information of the input images, the discriminative network discovers the regularizations of\n            the overall realness and segmentation consistency with the originals images. In the same\n            context and using GAN-based models for images inpainting, each work provides some prior\n            processing on GAN networks to get the best inpainting results for different types of images\n            including medical images [90], face images [91] or scenes images [92].\n\n          </p>\n\n            <p>\n            The GAN-based methods give a good addition to the performance of image inpainting\n            algorithms, but the speed of training is lower and needs very good performance machines,\n            and this is due to computational resources requirements including network parameters and\n            convolution operations.\n</p>\n\n          <p>There are many articles about application of Image Restoration\n            <a href=\"oldman.html\" target=\"_blank\">相关论文</a> You can have a look!</p>\n          \n              \n        <h2 id=\"tree\">3.Image Inpainting Datasets</h2>\n        \n            <p>Technique 1 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx</p>\n            <p>Technique 2 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx</p>\n            <p>Technique 3 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx</p>\n            <p>Technique 4 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx</p>\n\n        <img src=\"../assets/images/最后一张.png\" width=\"580\" height=\"320\">\n\n\n        <h2 id=\"example\">4.example of Image Inpainting</h2>\n        \n        <p>For more information <a href=\"https://en.wikipedia.org/wiki/Image_restoration\" target=\"_blank\">Visit Wikipedia!</a> </p>\n\n\n      </div>\n      </v-container>\n    </v-main>\n  </v-app>\n</template>\n\n<script>\n  import Header from '@/components/Header'\n\n  export default {\n    data() {\n      return {\n        headerTitles: [\n          'Page1',\n          'Page2',\n          'Page3',\n          'Page4'\n        ]\n      }\n    },\n    methods: {\n      test() {\n        // this.$refs.cards[\"1\"].text = \"Topic 1\";\n      }\n    },\n    components: {\n      Header,\n    },\n    mounted () {\n      // console.log(this.$refs.cards['1'])\n      // this.$refs.cards[\"1\"].text = \"Topic 1\";\n    }\n  }\n</script>","import mod from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Page2.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Page2.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./Page2.vue?vue&type=template&id=342d9142&\"\nimport script from \"./Page2.vue?vue&type=script&lang=js&\"\nexport * from \"./Page2.vue?vue&type=script&lang=js&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  null,\n  null\n  \n)\n\nexport default component.exports","module.exports = __webpack_public_path__ + \"img/CNN网络结构.745932b2.png\";","module.exports = __webpack_public_path__ + \"img/修复前.39d414b9.png\";","module.exports = __webpack_public_path__ + \"img/最后一张.3b87d2a9.png\";","module.exports = __webpack_public_path__ + \"img/修复后.9566e56e.png\";"],"sourceRoot":""}